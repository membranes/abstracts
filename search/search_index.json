{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"development-approach/","title":"Approach","text":"<p>For a specific token classification dependent problem, an artificial intelligence engineer can</p> <ul> <li>Develop a token classification model per language model architecture, e.g., RoBERTa (Robustly Optimized BERT Pretraining Approach), ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately).  Per architecture, hyperparameter optimisation/tuning techniques, and libraries, aid the development of quite effective models, subject to early stopping, etc., constraints.</li> <li>Select the best model amongst the set of models; a single model per architecture.</li> </ul> <p></p>"},{"location":"development-approach/#selecting-the-best-model","title":"Selecting the best model","text":"<p>For this exercise, the best model was selected by comparing a testing phase metric, specifically Matthews Correlation Coefficient (MCC):</p> \\[MCC = \\frac{(tn \\bullet tp) - (fn \\bullet fp)}{{\\Large{[}}(tp + fp)(tp + fn)(tn + fp)(tn + fn){\\Large{]}}^{0.5}}\\] \\[MCC \\in [-1, \\quad +1]\\] <p>wherein tn, tp, fn, and fp denote true negative, true positive, false negative, and false positive, respectively.</p> <p></p>"},{"location":"development-approach/#warning","title":"Warning","text":"<p>Note, the best model of a set must undergo (a) mathematical evaluation, and (b) business/cost evaluation.  The latter is critical because an acceptable mathematical metric, e.g., \\(precision &gt; 0.9\\) does not necessarily lead to excellent business/cost metrics.</p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"development-data/","title":"Data","text":""},{"location":"development-data/#sources","title":"Sources","text":"<p>Include:</p> <ul> <li>W-NUT 2017 (W-NUT 2017 Emerging and Rare entity recognition)<sup>1</sup>: Token Classification &amp; W-NUT 2017, get W-NUT 2017</li> <li>Few-NERD<sup>2</sup>: get Few-NERD</li> </ul> <p></p>"},{"location":"development-data/#annotating-tagging","title":"Annotating &amp; Tagging","text":"<p>Annotating, via annotation schemes, e.g., IOB, and tagging tools:</p> <ul> <li>Annotation Tools</li> <li>Doccano</li> <li>More</li> <li>doccano</li> <li>NER (Named Entity Recognition) Annotator</li> <li>Acharya for NER (Named Entity Recognition)</li> <li>gradio &amp; NER (Named Entity Recognition)</li> <li>tagtog</li> </ul> <p></p>"},{"location":"development-data/#extracting-text-from-documents","title":"Extracting text from documents","text":"<ul> <li>pypdf, pip</li> <li>example</li> <li>PyMuPDF, pip</li> <li>example</li> </ul> <ol> <li> <p>W-NUT: Workshop on Noisy User-generated Text \u21a9</p> </li> <li> <p>A Few-shot NERD (Named Entity Recognition Dataset) \u21a9</p> </li> </ol>"},{"location":"development-modelling/","title":"Model Development","text":""},{"location":"development-modelling/#hyperparameters","title":"Hyperparameters","text":"<p>Note RunConfig, CheckpointConfig, TuneConfig, ScalingConfig \\(\\in\\) ray.tune.run.</p> <ul> <li>Default Search Algorithm: BasicVariantGenerator</li> <li>Default Scheduler: FIFOScheduler</li> <li>trainer_utils</li> <li>default_compute_objective</li> <li>BestRun</li> <li>An example of next steps</li> <li>huggingface.co hyperparameter_search(...)</li> </ul> <p></p>"},{"location":"development-modelling/#tensorboard","title":"TensorBoard","text":"<p>The progress of a model training exercise is observable via TensorBoard</p> <pre><code>tensorboard --logdir \n    /tmp/ray/session_{datetime}_{host.code}/artifacts/{datetime}/tuning/driver_artifacts \n        --bind_all\n</code></pre> <p></p> <p>Subsequently, a link of the form <code>http://...:6007/</code> or <code>http://...:6006/</code> is printed.  Access the underlying pages via a browser.  It might be necessary to switch to <code>http://localhost:6007/</code> or <code>http://localhost:6006/</code>. <sup>2</sup></p> <p></p>"},{"location":"development-modelling/#computation-metrics-ray-prometheus-grafana","title":"Computation Metrics: Ray, Prometheus, Grafana","text":"<p>Via Ray Dashboard, aided by Prometheus &amp; Grafana<sup>3</sup>; the set-up for the latter pair is upcoming.  Ensure usage statistics sharing/collection is disabled.  Options</p> <ul> <li>os.environ['RAY_USAGE_STATS_ENABLED']='0'</li> <li>Either</li> <li>ray start --disable-usage-stats --head --dashboard-host=0.0.0.0 --dashboard-port=8265</li> <li>ray start --disable-usage-stats --head --dashboard-host=172.17.0.2 --dashboard-port=8265</li> <li>etc.</li> </ul> <p>The computation metrics will be accessible via * localhost:8265 * localhost:6379</p> <p></p>"},{"location":"development-modelling/#file-formats-of-developed-models","title":"File formats of developed models","text":"<ul> <li>GGUF: GPT-Generated Unified Format<sup>1</sup></li> <li>GGML: GPT-Generated Model Language</li> <li>What is GGUF and GGML?</li> <li>About GGUF</li> <li>to GGUF</li> <li>to GGUF discussion</li> <li>Hugging Face &amp; GGUF</li> </ul>"},{"location":"development-modelling/#steps-epochs","title":"Steps &amp; Epochs","text":"<p>The formulae in focus are</p> <ul> <li>max_steps_per_epoch = self.__source['train'].shape[0] // (variable.TRAIN_BATCH_SIZE * variable.N_GPU)</li> <li>max_steps = max_steps_per_epoch * self.__n_epochs</li> </ul> <p></p>"},{"location":"development-modelling/#classes-of-interest","title":"Classes of interest","text":"<ul> <li>transformers</li> <li>transformers.PreTrainedTokenizer</li> <li>serving</li> <li>pytorch DataLoader</li> <li>pytorch Dataset</li> <li>TokenClassifierOutput</li> <li>DatasetInfo, Dataset, etc</li> <li>Tensors</li> <li>For tensors that have a single value: torch.Tensor.item()</li> <li>Optimisation: torch.optim</li> <li>PretrainedConfig</li> </ul> <ol> <li> <p>GPT: Generative Pre-trained Transformer\u00a0\u21a9</p> </li> <li> <p>Using <code>CheckpointConfig.num_to_keep &lt;= 2</code> with PBT can lead to restoration problems when checkpoints are deleted too early, too early for other trials to exploit them. If this happens, increase the value of <code>num_to_keep</code>.\u00a0\u21a9</p> </li> <li> <p>Ray, Grafana, Prometheus \u21a9</p> </li> </ol>"},{"location":"preliminaries-aim/","title":"Aim","text":"<p>In brief.  Organisations identify or detect words/strings of interest within documents or data blobs for a variety of purposes, e.g., knowledge graph development, text redaction, criminal investigations, etc.  Organisations may opt for manual word detection if off-the-shelf solutions cannot address their needs.  For example, most redaction software solutions focus on detecting and redacting words/strings of a narrow set of categories, and it is rarely possible to adapt the solution to the detection of words/strings of bespoke categories.</p> <p></p> <p>A viable solution strategy involves the creation of custom models via named entity recognition machine learning algorithms; also known as token classification algorithms. These algorithms can be trained by domain or problem, leading to compact, domain specific, word/string detection models.  </p> <p></p> <p>This project publishes an adaptable token classification modelling set-up, i.e., artificial/machine learning engineers can fork and adapt the repositories to their \\((a)\\) token classification problem, \\((b)\\) development environment, and \\((c)\\) the range of language models that they would like to try.  The latter point is discussed below.</p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"preliminaries-application/","title":"Application","text":"<p>This section outlines a plausible \\((a)\\) problem statement, \\((b)\\) corresponding outcome expectations/underlying aims, and \\((c)\\) deployment goal vis-\u00e0-vis token classification model development.</p> <p></p>"},{"location":"preliminaries-application/#problem-statement","title":"Problem Statement","text":"<p>An organisation manually classifies trauma incidents for all the major trauma centres of five countries.  Per trauma case, an injury coding expert (a) examines the case's free and structured text, and assigns each piece of text to a category, and (b) assigns the case to a trauma category based on the combination text pieces &amp; categories detected; text pieces of the other/miscellaneous category are excluded from this exercise.  Trauma injury coding is an extremely intensive and time-consuming exercise, and injury coding error rates - per annum - can be quite high.  Hence, and as a first step, we are in search of a solution that automatically classifies text pieces vis-\u00e0-vis a set of provided categories.</p> <p></p>"},{"location":"preliminaries-application/#outcome-expectations-underlying-aims","title":"Outcome Expectations, Underlying Aims","text":"<ul> <li>Outcome Expectations: Real-time availability of classifications per trauma case.</li> <li>Underlying Aim: The automatic classification of trauma case text pieces; objective \u2192</li> <li>per case, automatic classification time &lt; 180 seconds.</li> <li>model metrics limits false negative rate \u2264 0.02, false positive rate \u2264 0.04</li> </ul>"},{"location":"preliminaries-application/#deployment-goal","title":"Deployment Goal","text":"<p>A potential machine learning dependent project without a deployment goal is directionless, do not proceed.  A plausible deployment goal is</p> <p></p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"preliminaries-assets/","title":"Assets","text":"<p>The membranes hub hosts the repositories of a token classification modelling project.  Readers may interact with the model via a simple open interface.</p>"},{"location":"preliminaries-assets/#repositories","title":"Repositories","text":"<p>The diagram outlines the functions of git repositories; hover over a git symbol for a brief summary of a repository's function.</p> <p></p> <p></p> <p>The <code>text</code> package will be re-written during the last quarter of 2025.  The restructuring repository package is for restructuring the latest data captures in preparation for (a) human inspection, and (b) usage as a model re-training data source.</p> <p></p> <p>Additionally</p> repositorycomment abstractsBackground, concept, etc. iacLimited infrastructure as code notes; intentionally opaque.  <p></p>"},{"location":"preliminaries-assets/#model-data","title":"Model &amp; Data","text":"<ul> <li>Model Description</li> <li>Data Description</li> </ul>"},{"location":"references/","title":"References","text":""},{"location":"references/#natural-language-processing","title":"Natural Language Processing","text":"<ul> <li>A Survey of Transformer-based Pretrained Models in Natural Language Processing</li> <li>A review of pretrained language models: from BERT, RoBERTa, to ELECTRA, DeBERTa, BigBird, and more</li> <li>Graph Transformers: A Survey</li> <li>A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</li> <li>Word Pieces: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</li> </ul>"},{"location":"references/#mathematics","title":"Mathematics","text":""},{"location":"references/#fundamentals","title":"Fundamentals","text":"<ul> <li>Minimisation Algorithms</li> <li>A simple illustration of likelihood functions in relation to based transformer models</li> <li>Pattern Recognition &amp; Machine Learning</li> <li>Deep Learning</li> </ul>"},{"location":"references/#transformers","title":"Transformers","text":"<ul> <li>A catalogue of transformer models</li> <li>Transformers: Attention is all you need: Re-visit this paper for a basic reminder of the underlying mathematics of BERT (Bidirectional Encoder Representations from Transformers)</li> <li>Annotated @ 2022</li> <li>Annotated @ 2018</li> <li>XLNet: Generalized Autoregressive Pretraining for Language Understanding</li> <li>DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</li> <li>ELECTRA</li> <li>Large Language Models: A Survey</li> <li>New LLM Pre-training and Post-training Paradigms</li> <li>A survey on recent advances in Named Entity Recognition</li> <li>END-TO-END NAMED ENTITY RECOGNITION AND RELATION EXTRACTION USING PRE-TRAINED LANGUAGE MODELS</li> <li>Knowledge graph extension with a pre-trained language model via unified learning method</li> <li>Graph Transformer Networks</li> <li>DistilBertForTokenClassification</li> </ul>"},{"location":"references/#research","title":"Research","text":"<ul> <li>A one-year-long research workshop on large multilingual models and datasets</li> <li>Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model</li> <li>Investigate Clay</li> <li>The State of AI in EO</li> <li>National Agriculture Imagery Program (NAIP) data embedded with Clay v1.5 (rev2)</li> <li>National Agriculture Imagery Program</li> <li>National Agriculture Imagery Program</li> <li>Bruno Sanchez-Andrade Nu\u00f1o</li> </ul>"},{"location":"references/#optimisation","title":"Optimisation","text":"<ul> <li>2.2 Transformer Model \\(\\odot\\) Position Information in Transformers: An Overview: Study this paper for an understanding of an unknown transformer model function.</li> <li>ADAM: Dive into Deep Learning</li> <li>ADAM: PyTorch</li> <li>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</li> <li>KLDivLoss</li> <li>Hyperparameter optimization: Foundations, algorithms,best practices, and open challenges</li> <li>Hyperparameters in Deep Learning: A Comprehensive Review</li> <li>Why Do We Need Weight Decay in Modern Deep Learning?</li> <li>Hyperparameter Optimization For LLMs: Advanced Strategies</li> <li>Population Based Training of Neural Networks</li> <li>Hyperparameter Search with the Huggingface Trainer</li> </ul>"},{"location":"references/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Freedom of information (FOI) Improvement Plan 2024</li> <li>Text Classification Notebook</li> <li>Using Huggingface Transformers with Tune, PBT</li> <li>Fine-tune a Hugging Face Transformers Model</li> <li>Tune Trial Schedulers (tune.schedulers)</li> <li>Tune Search Algorithms (tune.search)</li> <li>.hyperparameter_search(compute_objective=...)</li> <li>And much more.</li> <li>class</li> <li>cf.</li> <li>Analyzing Tune Experiment Results</li> <li>Get Started with Distributed Training using Hugging Face Transformers</li> <li>Visualizing Population Based Training (PBT) Hyperparameter Optimization</li> <li>Displaying Mathematics Formulae</li> </ul>"},{"location":"references/#interfaces","title":"Interfaces","text":""},{"location":"references/#gradio","title":"GRADIO","text":"<ul> <li>button</li> <li>space embedding</li> <li>gradio spaces</li> <li>entity recognition interface</li> </ul>"}]}